{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1. Import library:\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "stop_words = stopwords.words()"
      ],
      "metadata": {
        "id": "CVPSjL06TCmR",
        "outputId": "2b5d2672-17fb-4b4f-ae54-49d7d2f2d521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lYpdoMteTINt",
        "outputId": "a558890b-b2f8-43ba-bb8c-83bfb8e8fd1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the path to the CSV file in Google Drive\n",
        "file_path = '/content/Reviews.csv'\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "RBwxQI8lUFQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T\n"
      ],
      "metadata": {
        "id": "x37DU5olUlFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns\n",
        "print(column_names)\n"
      ],
      "metadata": {
        "id": "_cS4Tix5U-ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create subplots for reviews with 5-star and 1-star ratings\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
        "\n",
        "# Plot histogram for 5-star reviews (Positive reviews)\n",
        "ax1.hist(df[df['Score'] == 5]['Text'].str.len(), bins=30, color='green')\n",
        "ax1.set_title('5-Star Reviews')\n",
        "ax1.set_xlabel('Review Length')\n",
        "ax1.set_ylabel('Frequency')\n",
        "\n",
        "# Plot histogram for 1-star reviews (Negative reviews)\n",
        "ax2.hist(df[df['Score'] == 1]['Text'].str.len(), bins=30, color='red')\n",
        "ax2.set_title('1-Star Reviews')\n",
        "ax2.set_xlabel('Review Length')\n",
        "ax2.set_ylabel('Frequency')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "9cDtYTjEdW_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Score'].value_counts()\n"
      ],
      "metadata": {
        "id": "9WrmXnkrUtky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text'].str.len().hist()\n"
      ],
      "metadata": {
        "id": "S3Wy5DrRVL3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# استخراج نصوص المراجعات التي تحتوي على تقييم 5\n",
        "text = \" \".join(review for review in df[df['Score'] == 5]['Text'])\n",
        "\n",
        "# إنشاء سحابة الكلمات\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
        "\n",
        "# عرض سحابة الكلمات\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Wordcloud for 5-Star Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B664Sx-lXVhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# استخراج نصوص المراجعات التي تحتوي على تقييم 5\n",
        "text = \" \".join(review for review in df[df['Score'] == 5]['Text'])\n",
        "\n",
        "# إنشاء سحابة الكلمات\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
        "\n",
        "# عرض سحابة الكلمات\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title('Wordcloud for 5-Star Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g6IFUb1WG_ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p7ZNR4z1HNmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# دالة تنظيف النصوص\n",
        "def cleaning(text):\n",
        "    if isinstance(text, str):  # التحقق إذا كانت القيمة نصية\n",
        "        # تحويل الأحرف إلى أحرف صغيرة وإزالة الروابط والعلامات الخاصة وعلامات الترقيم\n",
        "        text = text.lower()  # تحويل النص إلى أحرف صغيرة\n",
        "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)  # إزالة الروابط\n",
        "        text = re.sub(r\"\\b\\d+\\b\", \"\", text)  # إزالة الأرقام\n",
        "        text = re.sub('<.*?>+', '', text)  # إزالة العلامات الخاصة\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # إزالة علامات الترقيم\n",
        "        text = re.sub('\\n', '', text)  # إزالة الانتقالات إلى السطر الجديد\n",
        "        text = re.sub('[’“”…]', '', text)  # إزالة بعض الرموز الخاصة\n",
        "\n",
        "        # إزالة الرموز التعبيرية\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # الرموز التعبيرية\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # الرموز والرموز التصويرية\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # رموز النقل والخرائط\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # الأعلام\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "        text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "        # توسيع الصيغ المختصرة:\n",
        "        text = re.sub(\"isn't\", 'is not', text)\n",
        "        text = re.sub(\"he's\", 'he is', text)\n",
        "        text = re.sub(\"wasn't\", 'was not', text)\n",
        "        text = re.sub(\"there's\", 'there is', text)\n",
        "        text = re.sub(\"couldn't\", 'could not', text)\n",
        "        text = re.sub(\"won't\", 'will not', text)\n",
        "        text = re.sub(\"they're\", 'they are', text)\n",
        "        text = re.sub(\"she's\", 'she is', text)\n",
        "        text = re.sub(\"There's\", 'there is', text)\n",
        "        text = re.sub(\"wouldn't\", 'would not', text)\n",
        "        text = re.sub(\"haven't\", 'have not', text)\n",
        "        text = re.sub(\"That's\", 'That is', text)\n",
        "        text = re.sub(\"you've\", 'you have', text)\n",
        "        text = re.sub(\"He's\", 'He is', text)\n",
        "        text = re.sub(\"what's\", 'what is', text)\n",
        "        text = re.sub(\"weren't\", 'were not', text)\n",
        "        text = re.sub(\"we're\", 'we are', text)\n",
        "        text = re.sub(\"hasn't\", 'has not', text)\n",
        "        text = re.sub(\"you'd\", 'you would', text)\n",
        "        text = re.sub(\"shouldn't\", 'should not', text)\n",
        "        text = re.sub(\"let's\", 'let us', text)\n",
        "        text = re.sub(\"they've\", 'they have', text)\n",
        "        text = re.sub(\"You'll\", 'You will', text)\n",
        "        text = re.sub(\"i'm\", 'i am', text)\n",
        "        text = re.sub(\"we've\", 'we have', text)\n",
        "        text = re.sub(\"it's\", 'it is', text)\n",
        "        text = re.sub(\"don't\", 'do not', text)\n",
        "        text = re.sub(\"that´s\", 'that is', text)\n",
        "        text = re.sub(\"I´m\", 'I am', text)\n",
        "        text = re.sub(\"it’s\", 'it is', text)\n",
        "        text = re.sub(\"she´s\", 'she is', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# تطبيق الدالة على عمود Text في البيانات\n",
        "df['cleaned_Text'] = df['Text'].apply(cleaning)\n",
        "\n"
      ],
      "metadata": {
        "id": "98DHlQMYHqbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'Text':'review'}, inplace = True)\n",
        "df"
      ],
      "metadata": {
        "id": "Fs43QENerI4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = pd.DataFrame(dt)\n",
        "dt['review']=df['review']\n",
        "dt"
      ],
      "metadata": {
        "id": "mjiMxruaMMbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter\n"
      ],
      "metadata": {
        "id": "Tl7QdgSHIoyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install modin[dask]\n"
      ],
      "metadata": {
        "id": "3cm2V1RzPZYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import modin.pandas as pd  # استبدل pandas بـ modin\n",
        "stop_words_set = set(stop_words)\n",
        "\n",
        "# استخدام modin بنفس الطريقة التي تستخدمها مع pandas\n",
        "dt['no_sw'] = dt['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words_set]))\n"
      ],
      "metadata": {
        "id": "AX75nc0uNehJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt"
      ],
      "metadata": {
        "id": "cfLdGIIuDuYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# عد الكلمات الأكثر تكرارًا في عمود 'no_sw' بعد إزالة الكلمات الشائعة\n",
        "cnt = Counter()\n",
        "for review in dt[\"no_sw\"].values:\n",
        "    for word in review.split():\n",
        "        cnt[word] += 1\n",
        "\n",
        "# الحصول على أكثر 10 كلمات تكرارًا\n",
        "most_common_words = cnt.most_common(10)\n",
        "\n",
        "# تحويل النتيجة إلى DataFrame\n",
        "temp = pd.DataFrame(most_common_words, columns=['word', 'count'])\n",
        "\n",
        "# عرض DataFrame الناتج\n",
        "print(temp)\n"
      ],
      "metadata": {
        "id": "OjpYHmsfQPu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.bar(temp, x=\"count\", y=\"word\", title='Commmon Words in review', orientation='h',\n",
        "             width=700, height=700)"
      ],
      "metadata": {
        "id": "1K7nGzcBW96M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the most frequent words:\n",
        "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
        "def remove_freqwords(text):\n",
        "    \"\"\"custom function to remove the frequent words\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
        "dt[\"wo_stopfreq\"] = dt[\"no_sw\"].apply(lambda review: remove_freqwords(review))\n",
        "dt.head()\n"
      ],
      "metadata": {
        "id": "1gRr4GTQAO78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# عرض العمود المعدل\n",
        "dt['no_sw'].loc[6]\n",
        "\n"
      ],
      "metadata": {
        "id": "NFCzPvptA4lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt['wo_stopfreq'].loc[6]\n"
      ],
      "metadata": {
        "id": "5nv77wUMFdy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "HBP6ed9TRP62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# استيراد مكتبة WordNetLemmatizer من مكتبة NLTK\n",
        "# WordNetLemmatizer هو كائن يستخدم لتحويل الكلمات إلى شكلها الأساسي (lemma).\n",
        "wordnet_lem = WordNetLemmatizer()\n",
        "\n",
        "# تطبيق التمثيل الأساسي (Lemmatization) على النصوص في عمود \"wo_stopfreq\"\n",
        "# هذا الكود يقوم بتحويل كل كلمة في النص إلى شكلها الأساسي\n",
        "dt['wo_stopfreq_lem'] = dt['wo_stopfreq'].apply(wordnet_lem.lemmatize)\n",
        "\n",
        "# عرض إطار البيانات بعد إضافة العمود الجديد \"wo_stopfreq_lem\"\n",
        "# هذا العمود يحتوي على النصوص بعد إزالة الكلمات المتكررة وتطبيق التمثيل الأساسي\n",
        "dt\n"
      ],
      "metadata": {
        "id": "mY9kSqu_G-J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# افترض أن dt هي DataFrame موجودة مسبقًا\n",
        "nb = dt.drop(columns=[ 'no_sw', 'wo_stopfreq'])  # حذف الأعمدة غير المرغوب فيها\n",
        "\n"
      ],
      "metadata": {
        "id": "NMTV3ayGIOd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_review=nb['review'].apply(lambda x: x.split())\n",
        "tokenized_review.head(5)"
      ],
      "metadata": {
        "id": "c1EL3iC4UKEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts = cv.fit_transform(nb['review'])"
      ],
      "metadata": {
        "id": "uu0K4TiJUScO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تحويل قيم sentiment إلى 0 (سلبية) و1 (إيجابية)\n",
        "nb.Score = [0 if each <= 2 else 1 for each in nb.Score]\n",
        "\n"
      ],
      "metadata": {
        "id": "kQUPVXeRhH9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# استيراد المكتبات اللازمة\n",
        "import pandas as pd  # مكتبة التعامل مع البيانات\n",
        "from sklearn.naive_bayes import BernoulliNB  # نموذج Bernoulli Naive Bayes\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # تقييم النموذج\n",
        "from sklearn.model_selection import train_test_split  # تقسيم البيانات"
      ],
      "metadata": {
        "id": "DFGuT3E6XUhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=text_counts\n",
        "y=nb['Score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=30)"
      ],
      "metadata": {
        "id": "todW7ogFWE3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "CNB = ComplementNB()\n",
        "CNB.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "9Nk-dcwDeYM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "predicted = CNB.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
        "\n",
        "print('ComplementNB model accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')\n",
        "print('------------------------------------------------')\n",
        "print(pd.DataFrame(confusion_matrix(y_test, predicted)))\n",
        "print('------------------------------------------------')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, predicted))"
      ],
      "metadata": {
        "id": "Ofw0wHIBejRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(X_train, y_train)\n",
        "\n",
        "predicted = MNB.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted, y_test)\n"
      ],
      "metadata": {
        "id": "knE0MyZde5Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(X_train, y_train)\n",
        "\n",
        "predicted = MNB.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
        "\n",
        "print('MultinominalNB model accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')\n",
        "print('------------------------------------------------')\n",
        "print('Confusion Matrix:')\n",
        "print(pd.DataFrame(confusion_matrix(y_test, predicted)))\n",
        "print('------------------------------------------------')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, predicted))"
      ],
      "metadata": {
        "id": "oWcGW99Ue_pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "BNB.fit(X_train, y_train)\n",
        "\n",
        "predicted = BNB.predict(X_test)\n",
        "accuracy_score_bnb = metrics.accuracy_score(predicted,y_test)\n",
        "\n",
        "print('BernoulliNB model accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "print('------------------------------------------------')\n",
        "print('Confusion Matrix:')\n",
        "print(pd.DataFrame(confusion_matrix(y_test, predicted)))\n",
        "print('------------------------------------------------')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, predicted))"
      ],
      "metadata": {
        "id": "zDs8Kv5bfGRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn\n"
      ],
      "metadata": {
        "id": "aDXkCr-kfbjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# predict probabilities for CNB, MNB, BNB models:\n",
        "CNB_prob = CNB.predict_proba(X_test)\n",
        "MNB_prob = MNB.predict_proba(X_test)\n",
        "BNB_prob = BNB.predict_proba(X_test)\n",
        "\n",
        "# roc curve for models\n",
        "fpr1, tpr1, thresh1 = roc_curve(y_test, CNB_prob[:,1], pos_label=1)\n",
        "fpr2, tpr2, thresh2 = roc_curve(y_test, MNB_prob[:,1], pos_label=1)\n",
        "fpr3, tpr3, thresh3 = roc_curve(y_test, BNB_prob[:,1], pos_label=1)\n",
        "\n",
        "# roc curve for tpr = fpr\n",
        "random_probs = [0 for i in range(len(y_test))]\n",
        "p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
        "\n",
        "\n",
        "# auc scores\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc_CNB = roc_auc_score(y_test, CNB_prob[:,1])\n",
        "auc_MNB = roc_auc_score(y_test, MNB_prob[:,1])\n",
        "auc_BNB = roc_auc_score(y_test, BNB_prob[:,1])\n",
        "print(auc_CNB, auc_MNB,auc_BNB)\n",
        "\n",
        "# plot roc curves\n",
        "plt.plot(fpr1, tpr1, linestyle='--',color='red', label='CNB Model')\n",
        "plt.plot(fpr2, tpr2, linestyle='--',color='green', label='MNB Model')\n",
        "plt.plot(fpr3, tpr3, linestyle='--',color='blue', label='BNB Model')\n",
        "plt.plot(p_fpr, p_tpr, linestyle='--', color='pink')\n",
        "\n",
        "# title\n",
        "plt.title('ROC curve')\n",
        "# x label\n",
        "plt.xlabel('False Positive Rate')\n",
        "# y label\n",
        "plt.ylabel('True Positive rate')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('ROC',dpi=300)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "5TAwGf-VfJUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv_twograms = CountVectorizer(stop_words='english',ngram_range = (2,2),tokenizer = token.tokenize)\n",
        "text_counts_twograms = cv_twograms.fit_transform(nb['review'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_counts_twograms, nb['Score'], test_size=0.20,random_state=30)\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(X_train, y_train)\n",
        "predicted = MNB.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted, y_test)\n",
        "print('model accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')"
      ],
      "metadata": {
        "id": "wrTCJ0GTfMQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "text_count_2 = tfidf.fit_transform(nb['review'])"
      ],
      "metadata": {
        "id": "z7wMqghdjCl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting the model with MNB\n",
        "MNB.fit(X_train, y_train)\n",
        "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(X_test), y_test)\n",
        "\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')"
      ],
      "metadata": {
        "id": "O3OL7id5kmqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting the model with BNB\n",
        "BNB.fit(X_train, y_train)\n",
        "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(X_test), y_test)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')"
      ],
      "metadata": {
        "id": "h8A_o3R9k5_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}